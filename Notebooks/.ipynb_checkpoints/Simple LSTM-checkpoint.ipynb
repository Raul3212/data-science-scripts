{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raula\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2ca8263d52af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municode_codes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUNICODE_EMOJI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/dataset2018.tsv', header=0, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['classe'] != 'N√£o sei']\n",
    "data['classe'][data['classe'] == 'Rejei√ß√£o'] = 2\n",
    "data['classe'][data['classe'] == 'Neutro'] = 1\n",
    "data['classe'][data['classe'] == 'Aprova√ß√£o'] = 0\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(twitterText):\n",
    "    #Remover \\n\n",
    "    twitterText = re.sub(\"\\n+\",\" \",twitterText)\n",
    "\n",
    "    #Remover multiplos espa√ßos\n",
    "    twitterText = re.sub(\" +\",\" \",twitterText)\n",
    "    \n",
    "    #(@usu√°rio) pelo termo ‚ÄôAT_USER‚Äô tal como sugerido em [Almatrafi et al., 2015].\n",
    "    twitterText = re.sub(\"@\\w+\",\"atuser\",twitterText)\n",
    "\n",
    "    #Remove links\n",
    "    twitterText = re.sub(r\"http\\S+\", \"\",twitterText)\n",
    "\n",
    "    #Remover caracteres especiais\n",
    "    twitterText = re.sub(\"[@|#|‚Äú|‚Äù|‚Äô|‚Äò|¬Æ|,|!|?||\\[|\\]|\\.|\\\"|%|:|\\-|_|/|¬™|\\(|\\)|¬∞|\\*|üáß|üá∑|\\'|Ô∏è|=]\",'',twitterText)\n",
    "\n",
    "    #Remover n√∫meros\n",
    "    twitterText = re.sub(\"[0-9]+\",'',twitterText)\n",
    "\n",
    "    #Tokenize\n",
    "    twitterTokens = TweetTokenizer().tokenize(twitterText)\n",
    "\n",
    "    #transforme emojis em textcode\n",
    "    twitterTokensEmojisCode = []\n",
    "    for token in twitterTokens:\n",
    "        if(token in UNICODE_EMOJI):\n",
    "            twitterTokensEmojisCode.append(UNICODE_EMOJI[token])\n",
    "        else:\n",
    "            twitterTokensEmojisCode.append(token)\n",
    "    twitterTokens = twitterTokensEmojisCode\n",
    "\n",
    "    #remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    stopwords.remove(\"n√£o\")\n",
    "    stopwords.remove(\"num\")\n",
    "    twitterTokens = [token for token in twitterTokens if (token not in stopwords) ]\n",
    "    \n",
    "    #Lower case\n",
    "    twitterText = \"\".join(twitterText)\n",
    "    twitterText = twitterText.lower()\n",
    "\n",
    "    return twitterText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : preProcessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['tweet']\n",
    "Y = data['classe'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply w2vec em X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGeneratorMedia:\n",
    "    def __init__(self, X, w2vmodel, num_features):\n",
    "        self.X = X\n",
    "        self.w2vmodel = w2vmodel\n",
    "        self.num_features = num_features\n",
    "        self.features_vec = None\n",
    "\n",
    "    def gen_features_dataset(self):\n",
    "        self.X = self.X.apply(lambda text: TweetTokenizer().tokenize(text) )\n",
    "        X_array = []\n",
    "        self.X.apply(lambda listText: X_array.append(self.make_features_vec(listText)) )\n",
    "        \n",
    "        return np.matrix(X_array)\n",
    "        \n",
    "    def make_features_vec(self, tweet):\n",
    "        featureVec = np.zeros(self.num_features)\n",
    "        nwords = 0.0\n",
    "        index2word_set = set(self.w2vmodel.wv.index2word)\n",
    "        for word in tweet:\n",
    "            if word in index2word_set:\n",
    "                featureVec = np.add(featureVec, self.w2vmodel[word])\n",
    "                nwords += 1\n",
    "        if nwords == 0.0:\n",
    "            nwords = 1.0\n",
    "        return np.divide(featureVec, nwords)\n",
    "\n",
    "def featureextractionWord2VecMean(X):\n",
    "    num_features=300\n",
    "    model = word2vec.Word2Vec.load(\"word2vec/tweets_presidential_elections_300_min1_cont2_cbow\")\n",
    "    featureGeneratorMedia = FeatureGeneratorMedia(X,model,num_features)\n",
    "    return featureGeneratorMedia.gen_features_dataset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featureextractionWord2VecMean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vec/tweets_presidential_elections_300_min1_cont2_cbow\")\n",
    "vocab_size = len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_dim=300\n",
    "output_dim=3\n",
    "lstm_dim=300\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,w2vec_dim,trainable=True))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(LSTM(lstm_dim))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "model.compile('adam', 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
